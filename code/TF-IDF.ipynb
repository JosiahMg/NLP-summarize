{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF 词频-逆文本频率\n",
    "\n",
    "### TF(Term Frequency),\n",
    "衡量⼀个term在⽂档中出现得有多频繁。  \n",
    "TF(t) = (t出现在⽂档中的次数) / (⽂档中的term总数).\n",
    "\n",
    "###  IDF: Inverse Document Frequency\n",
    "衡量⼀个term有多重要。\n",
    "\n",
    "有些词出现的很多，但是明显不是很有卵⽤。⽐如’is'，’the‘，’and‘之类\n",
    "的。\n",
    "为了平衡，我们把罕见的词的重要性（weight）搞⾼，\n",
    "把常见词的重要性搞低。\n",
    "IDF(t) = log_e(⽂档总数 / 含有t的⽂档总数).  \n",
    "\n",
    "### TF-IDF = TF * IDF\n",
    "\n",
    "# 举例\n",
    "\n",
    "⼀个⽂档有100个单词，其中单词baby出现了3次。  \n",
    "那么，TF(baby) = (3/100) = 0.03.  \n",
    "\n",
    "现在我们如果有10M的⽂档， baby出现在其中的1000个⽂档中。  \n",
    "那么，IDF(baby) = log(10,000,000 / 1,000) = 4  \n",
    "\n",
    "\n",
    "所以， TF-IDF(baby) = TF(baby) * IDF(baby) = 0.03 * 4 = 0.12  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## python实现TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
      "[('to', 0.0322394037469742), ('stop', 0.0322394037469742), ('worthless', 0.0322394037469742), ('my', 0.028288263356383563), ('dog', 0.028288263356383563), ('him', 0.028288263356383563), ('stupid', 0.028288263356383563), ('has', 0.025549122992281622), ('flea', 0.025549122992281622), ('problems', 0.025549122992281622), ('help', 0.025549122992281622), ('please', 0.025549122992281622), ('maybe', 0.025549122992281622), ('not', 0.025549122992281622), ('take', 0.025549122992281622), ('park', 0.025549122992281622), ('dalmation', 0.025549122992281622), ('is', 0.025549122992281622), ('so', 0.025549122992281622), ('cute', 0.025549122992281622), ('I', 0.025549122992281622), ('love', 0.025549122992281622), ('posting', 0.025549122992281622), ('garbage', 0.025549122992281622), ('mr', 0.025549122992281622), ('licks', 0.025549122992281622), ('ate', 0.025549122992281622), ('steak', 0.025549122992281622), ('how', 0.025549122992281622), ('quit', 0.025549122992281622), ('buying', 0.025549122992281622), ('food', 0.025549122992281622)]\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import operator\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "函数说明:创建数据样本\n",
    "Returns:\n",
    "    dataset - 实验样本切分的词条\n",
    "    classVec - 类别标签向量\n",
    "\"\"\"\n",
    "\n",
    "def loadDataSet():\n",
    "    dataset = [ ['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],    # 切分的词条\n",
    "                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid'] ]\n",
    "    classVec = [0, 1, 0, 1, 0, 1]  # 类别标签向量，1代表好，0代表不好\n",
    "    return dataset, classVec\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "函数说明：特征选择TF-IDF算法\n",
    "Parameters:\n",
    "     list_words:词列表\n",
    "Returns:\n",
    "     dict_feature_select:特征选择词字典\n",
    "\"\"\"\n",
    "\n",
    "def feature_select(list_words):\n",
    "    #总词频统计\n",
    "    doc_frequency=defaultdict(int)\n",
    "    for word_list in list_words:\n",
    "        for i in word_list:\n",
    "            doc_frequency[i]+=1\n",
    " \n",
    "    #计算每个词的TF值\n",
    "    word_tf={}  #存储没个词的tf值\n",
    "    for i in doc_frequency:\n",
    "        word_tf[i]=doc_frequency[i]/sum(doc_frequency.values())\n",
    " \n",
    "    #计算每个词的IDF值\n",
    "    doc_num=len(list_words)\n",
    "    word_idf={} #存储每个词的idf值\n",
    "    word_doc=defaultdict(int) #存储包含该词的文档数\n",
    "    for i in doc_frequency:\n",
    "        for j in list_words:\n",
    "            if i in j:\n",
    "                word_doc[i]+=1\n",
    "    for i in doc_frequency:\n",
    "        word_idf[i]=math.log(doc_num/(word_doc[i]+1))\n",
    " \n",
    "    #计算每个词的TF*IDF的值\n",
    "    word_tf_idf={}\n",
    "    for i in doc_frequency:\n",
    "        word_tf_idf[i]=word_tf[i]*word_idf[i]\n",
    " \n",
    "    # 对字典按值由大到小排序\n",
    "    dict_feature_select=sorted(word_tf_idf.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    return dict_feature_select\n",
    " \n",
    "if __name__=='__main__':\n",
    "    data_list, label_list = loadDataSet() #加载数据\n",
    "    features = feature_select(data_list) #所有词的TF-IDF值\n",
    "    print(features)\n",
    "    print(len(features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK实现TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is', 'sentence', 'one'], ['this', 'is', 'sentence', 'two'], ['this', 'is', 'sentence', 'three']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.21972245773362198"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.text import TextCollection\n",
    "import nltk\n",
    "\n",
    "sents = ['this is sentence one', 'this is sentence two', 'this is sentence three']\n",
    "\n",
    "sents = [nltk.word_tokenize(sent) for sent in sents]\n",
    "print(sents)\n",
    "\n",
    "corpus = TextCollection(sents)\n",
    "\n",
    "# 计算IDF\n",
    "corpus.idf('this') # log_e(3/3) = 0\n",
    "corpus.idf('three') # log_e(3/1)=1.0986\n",
    "\n",
    "# 计算TF\n",
    "corpus.tf('three', nltk.word_tokenize('one two three, go')) # 1/5\n",
    "corpus.tf('three', 'one two three, go')  # 1/17 this is wrong\n",
    "\n",
    "#计算tf-idf\n",
    "corpus.tf_idf('three', nltk.word_tokenize('one two three, go')) # 1/5 * log_e(3/1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn实现TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.41285857 0.69903033 0.41285857 0.41285857 0.         0.        ]\n",
      " [0.41285857 0.         0.41285857 0.41285857 0.         0.69903033]\n",
      " [0.41285857 0.         0.41285857 0.41285857 0.69903033 0.        ]]\n",
      "['is', 'one', 'sentence', 'this', 'three', 'two']\n",
      "this 3\n",
      "is 0\n",
      "sentence 2\n",
      "one 1\n",
      "two 5\n",
      "three 4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(smooth_idf=True)\n",
    "# print(tfidf)\n",
    "\n",
    "corpus_en = ['this is sentence one', 'this is sentence two', 'this is sentence three']\n",
    "\n",
    "# 分词后的中文语料库\n",
    "corpus_cn = ['我 来到 北京大学', '他 来到 了 网易 行研 大厦', '小明 硕士 毕业 于 中国 科学院', '我 爱 北京天安门']\n",
    "\n",
    "\n",
    "# tfidf的词向量\n",
    "result_cn = tfidf.fit_transform(corpus_en).toarray()\n",
    "print(result_cn)\n",
    "\n",
    "# 关键词\n",
    "word = tfidf.get_feature_names()\n",
    "print(word)\n",
    "\n",
    "stopwords = tfidf.get_stop_words()\n",
    "\n",
    "# 统计关键词\n",
    "for k, v in tfidf.vocabulary_.items():\n",
    "    print(k, v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'one', 'sentence', 'this', 'three', 'two']\n",
      "[[1 1 1 1 0 0]\n",
      " [1 0 1 1 0 1]\n",
      " [1 0 1 1 1 0]]\n",
      "[[0.41285857 0.69903033 0.41285857 0.41285857 0.         0.        ]\n",
      " [0.41285857 0.         0.41285857 0.41285857 0.         0.69903033]\n",
      " [0.41285857 0.         0.41285857 0.41285857 0.69903033 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "corpus = ['aaa ccc aaa aaa', \n",
    "          'aaa aaa', \n",
    "          'aaa aaa aaa', \n",
    "          'aaa aaa aaa aaa',\n",
    "          'aaa bbb aaa bbb aaa',\n",
    "          'ccc aaa aaa ccc aaa'\n",
    "         ]\n",
    "\n",
    "corpus_en = ['this is sentence one', 'this is sentence two', 'this is sentence three']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus_en)\n",
    "\n",
    "# 获取词袋模型中的所有词语   \n",
    "word = vectorizer.get_feature_names()  \n",
    "print(word) \n",
    "\n",
    "# 获取每个词在该行（文档）中出现的次数\n",
    "counts =  X.toarray()\n",
    "print (counts)\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(X)\n",
    "#tfidf = transformer.fit_transform(counts) #与上一行的效果完全一样\n",
    "#print(tfidf)\n",
    "print(tfidf.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jieba实现TF-IDF\n",
    "\n",
    "jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())  \n",
    "sentence 为待提取的文本  \n",
    "topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20  \n",
    "withWeight 为是否一并返回关键词权重值，默认值为 False  \n",
    "allowPOS 仅包括指定词性的词，默认值为空，即不筛选  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\86153\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.647 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['文档', '文本', '关键词', '挖掘', '文本检索']\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse\n",
    "\n",
    "text = '关键词是能够表达文档中心内容的词语，常用于计算机系统标引论文内容特征、\\\n",
    "信息检索、系统汇集以供读者检阅。关键词提取是文本挖掘领域的一个分支，是文本检索、\\\n",
    "文档比较、摘要生成、文档分类和聚类等文本挖掘研究的基础性工作'\n",
    "\n",
    "keywords=jieba.analyse.extract_tags(text, topK=5, withWeight=False, allowPOS=())\n",
    "print(keywords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
