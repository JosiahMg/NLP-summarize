{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词袋模型\n",
    "\n",
    " 将所有词语装进一个袋子里，不考虑其词法和语序的问题，即每个词语都是独立的。\n",
    " \n",
    " 例句:\n",
    "\n",
    "Jane wants to go to Shenzhen.\n",
    "\n",
    "Bob  wants to go to Shanghai.\n",
    "\n",
    "根据上面的两句制作袋子，里包括Jane、wants、to、go、Shenzhen、Bob、Shanghai。假设建立一个数组（或词典）用于映射匹配。  \n",
    "词典：[Jane, wants, to, go, Shenzhen, Bob, Shanghai]\n",
    "\n",
    "根据上面的词典，对例句建立词袋模型(统计个数)如下：  \n",
    "例句1 ： [1,1,2,1,1,0,0]  \n",
    "例句2 ： [0,1,2,1,0,1,1]  \n",
    "\n",
    "根据上面的词典，对例句建立词袋模型(不统计个数)如下：  \n",
    "例句1 ： [1,1,1,1,1,0,0]  \n",
    "例句2 ： [0,1,1,1,0,1,1]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用keras建立词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "# i创建一个分词器（tokenizer），设置为只考虑前1000个最常见的单词\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "\n",
    "# 构建索引单词\n",
    "tokenizer.fit_on_texts(samples)\n",
    "\n",
    "# 将字符串转换为整数索引组成的列表\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode=\"binary\")\n",
    "print(one_hot_results)\n",
    "\n",
    "# 得到单词索引\n",
    "word_index = tokenizer.word_index  \n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用nltk建立词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "\n",
    "corpus = 'this is my sentence ' \\\n",
    " 'this is my life ' \\\n",
    " 'this is the day'\n",
    "\n",
    "tokens = nltk.word_tokenize(corpus)\n",
    "\n",
    "# 统计词频\n",
    "fdist = FreqDist(tokens)\n",
    "# 打印'is'出现的次数\n",
    "print(fdist['is'])\n",
    "\n",
    "# 列举出现最多的50个词,虽然句子没有50个词\n",
    "standard_freq_vector = fdist.most_common(50)\n",
    "# 统计实际多少个词\n",
    "size = len(standard_freq_vector)\n",
    "print(standard_freq_vector)\n",
    "\n",
    "# 按照出来的频率记录每个单词的位置\n",
    "\n",
    "def position_lookup(v):\n",
    "    res = {}\n",
    "    counter = 0\n",
    "    for word in v:\n",
    "        res[word[0]] = counter\n",
    "        counter += 1\n",
    "    return res\n",
    "\n",
    "# 生成standard_freq_vector的位置字典\n",
    "standard_position_dict = position_lookup(standard_freq_vector)\n",
    "print(standard_position_dict)\n",
    "\n",
    "# 新句子\n",
    "sentence = 'this is cool'\n",
    "# 先新建⼀个跟我们的词典同样⼤⼩的向量\n",
    "freq_vector = [0] * size\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "for word in tokens:\n",
    "    try:\n",
    "        freq_vector[standard_position_dict[word]] += 1\n",
    "    except KeyError: #如果遇到新词则进入,如cool\n",
    "        continue\n",
    "\n",
    "print(freq_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用普通python语法对诗句进行词袋模型的建立"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# 设置词向量的空间，即每首诗的维度\n",
    "max_words = 1000\n",
    "\n",
    "\n",
    "# 读取txt中的诗句，将诗的题目去除\n",
    "# poetrys为列表类型，每个成员为每首诗\n",
    "poetrys = [line.strip().replace(' ', '').split(':')[1] for line in open('../corpus/poetry.txt', encoding='utf-8')]\n",
    "\n",
    "# 将所有诗句分割成单个词放入列表words中\n",
    "words = []\n",
    "for poetry in poetrys:\n",
    "    words += [word for word in poetry]\n",
    "\n",
    "# 统计所有词频, counter为字典类型，key=单个词， value为单个词出现的次数\n",
    "counter = collections.Counter(words)\n",
    "\n",
    "# 按照从大到小的顺序排名\n",
    "count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "\n",
    "# words为元组类型\n",
    "words, _ = zip(*count_pairs)\n",
    "\n",
    "\n",
    "# 取出最多1000个词组成字典, 并将最后一个词设置为*, 即不在字典中的字用'*'代替\n",
    "words_size = min(max_words, len(words))\n",
    "words = words[:words_size] + ('*',)\n",
    "words_size = len(words)\n",
    "\n",
    "# 将字符映射成字典， key为词， value为词的编号0-1000\n",
    "char2id_dict = {w: i for i, w in enumerate(words)}\n",
    "\n",
    "# 打印未知字符'*'的编号 为 1000\n",
    "unknow_char = char2id_dict.get('*')\n",
    "\n",
    "# 定义char2id函数，用于查找字符的索引，找不到则返回1000\n",
    "char2id = lambda char: char2id_dict.get(char, unknow_char)\n",
    "\n",
    "# 根据诗句的长度进行排序\n",
    "poetrys = sorted(poetrys, key=lambda line: len(line))\n",
    "\n",
    "# 将诗句使用字典的索引将汉字替换成数字\n",
    "poetrys_index_vector = [list(map(char2id, poetry)) for poetry in poetrys]\n",
    "\n",
    "# 每首诗使用维度为1000的向量表示，在字典对应位置使用词频表示\n",
    "# 并append到poetrys_vector列表中\n",
    "poetrys_vector = []\n",
    "for poetry in poetrys:\n",
    "    poetry_vector = [0]*words_size\n",
    "    for word in poetry:\n",
    "        try:\n",
    "            poetry_vector[char2id_dict[word]] += 1\n",
    "        except KeyError: #如果遇到新词则进入\n",
    "            poetry_vector[unknow_char] += 1\n",
    "    poetrys_vector.append(poetry_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
