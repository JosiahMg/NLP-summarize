{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词袋模型\n",
    "\n",
    " 将所有词语装进一个袋子里，不考虑其词法和语序的问题，即每个词语都是独立的。\n",
    " \n",
    " 例句:\n",
    "\n",
    "Jane wants to go to Shenzhen.\n",
    "\n",
    "Bob  wants to go to Shanghai.\n",
    "\n",
    "根据上面的两句制作袋子，里包括Jane、wants、to、go、Shenzhen、Bob、Shanghai。假设建立一个数组（或词典）用于映射匹配。  \n",
    "词典：[Jane, wants, to, go, Shenzhen, Bob, Shanghai]\n",
    "\n",
    "根据上面的词典，对例句建立词袋模型(统计个数)如下：  \n",
    "例句1 ： [1,1,2,1,1,0,0]  \n",
    "例句2 ： [0,1,2,1,0,1,1]  \n",
    "\n",
    "根据上面的词典，对例句建立词袋模型(不统计个数)如下：  \n",
    "例句1 ： [1,1,1,1,1,0,0]  \n",
    "例句2 ： [0,1,1,1,0,1,1]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用keras建立词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n",
      "{'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "# i创建一个分词器（tokenizer），设置为只考虑前1000个最常见的单词\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "\n",
    "# 构建索引单词\n",
    "tokenizer.fit_on_texts(samples)\n",
    "\n",
    "# 将字符串转换为整数索引组成的列表\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode=\"binary\")\n",
    "print(one_hot_results)\n",
    "\n",
    "# 得到单词索引\n",
    "word_index = tokenizer.word_index  \n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用nltk建立词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "\n",
    "corpus = 'this is my sentence ' \\\n",
    " 'this is my life ' \\\n",
    " 'this is the day'\n",
    "\n",
    "tokens = nltk.word_tokenize(corpus)\n",
    "\n",
    "# 统计词频\n",
    "fdist = FreqDist(tokens)\n",
    "# 打印'is'出现的次数\n",
    "print(fdist['is'])\n",
    "\n",
    "# 列举出现最多的50个词,虽然句子没有50个词\n",
    "standard_freq_vector = fdist.most_common(50)\n",
    "# 统计实际多少个词\n",
    "size = len(standard_freq_vector)\n",
    "print(standard_freq_vector)\n",
    "\n",
    "# 按照出来的频率记录每个单词的位置\n",
    "\n",
    "def position_lookup(v):\n",
    "    res = {}\n",
    "    counter = 0\n",
    "    for word in v:\n",
    "        res[word[0]] = counter\n",
    "        counter += 1\n",
    "    return res\n",
    "\n",
    "# 生成standard_freq_vector的位置字典\n",
    "standard_position_dict = position_lookup(standard_freq_vector)\n",
    "print(standard_position_dict)\n",
    "\n",
    "# 新句子\n",
    "sentence = 'this is cool'\n",
    "# 先新建⼀个跟我们的词典同样⼤⼩的向量\n",
    "freq_vector = [0] * size\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "for word in tokens:\n",
    "    try:\n",
    "        freq_vector[standard_position_dict[word]] += 1\n",
    "    except KeyError: #如果遇到新词则进入,如cool\n",
    "        continue\n",
    "\n",
    "print(freq_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用普通python语法对诗句进行词袋模型的建立"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# 设置词向量的空间，即每首诗的维度\n",
    "max_words = 1000\n",
    "\n",
    "\n",
    "# 读取txt中的诗句，将诗的题目去除\n",
    "# poetrys为列表类型，每个成员为每首诗\n",
    "poetrys = [line.strip().replace(' ', '').split(':')[1] for line in open('../corpus/poetry.txt', encoding='utf-8')]\n",
    "\n",
    "# 将所有诗句分割成单个词放入列表words中\n",
    "words = []\n",
    "for poetry in poetrys:\n",
    "    words += [word for word in poetry]\n",
    "\n",
    "# 统计所有词频, counter为字典类型，key=单个词， value为单个词出现的次数\n",
    "counter = collections.Counter(words)\n",
    "\n",
    "# 按照从大到小的顺序排名\n",
    "count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "\n",
    "# words为元组类型\n",
    "words, _ = zip(*count_pairs)\n",
    "\n",
    "\n",
    "# 取出最多1000个词组成字典, 并将最后一个词设置为*, 即不在字典中的字用'*'代替\n",
    "words_size = min(max_words, len(words))\n",
    "words = words[:words_size] + ('*',)\n",
    "words_size = len(words)\n",
    "\n",
    "# 将字符映射成字典， key为词， value为词的编号0-1000\n",
    "char2id_dict = {w: i for i, w in enumerate(words)}\n",
    "\n",
    "# 打印未知字符'*'的编号 为 1000\n",
    "unknow_char = char2id_dict.get('*')\n",
    "\n",
    "# 定义char2id函数，用于查找字符的索引，找不到则返回1000\n",
    "char2id = lambda char: char2id_dict.get(char, unknow_char)\n",
    "\n",
    "# 根据诗句的长度进行排序\n",
    "poetrys = sorted(poetrys, key=lambda line: len(line))\n",
    "\n",
    "# 将诗句使用字典的索引将汉字替换成数字\n",
    "poetrys_index_vector = [list(map(char2id, poetry)) for poetry in poetrys]\n",
    "\n",
    "# 每首诗使用维度为1000的向量表示，在字典对应位置使用词频表示\n",
    "# 并append到poetrys_vector列表中\n",
    "poetrys_vector = []\n",
    "for poetry in poetrys:\n",
    "    poetry_vector = [0]*words_size\n",
    "    for word in poetry:\n",
    "        try:\n",
    "            poetry_vector[char2id_dict[word]] += 1\n",
    "        except KeyError: #如果遇到新词则进入\n",
    "            poetry_vector[unknow_char] += 1\n",
    "    poetrys_vector.append(poetry_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "import jieba\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "载入中文数据以及对应的包，corpora是构造词典的, similarities求相似性可以用得到\n",
    "\"\"\"\n",
    "\n",
    "raw_documents = [  \n",
    "    '0无偿居间介绍买卖毒品的行为应如何定性',  \n",
    "    '1吸毒男动态持有大量毒品的行为该如何认定',  \n",
    "    '2如何区分是非法种植毒品原植物罪还是非法制造毒品罪',  \n",
    "    '3为毒贩贩卖毒品提供帮助构成贩卖毒品罪',  \n",
    "    '4将自己吸食的毒品原价转让给朋友吸食的行为该如何认定',  \n",
    "    '5为获报酬帮人购买毒品的行为该如何认定',  \n",
    "    '6毒贩出狱后再次够买毒品途中被抓的行为认定',  \n",
    "    '7虚夸毒品功效劝人吸食毒品的行为该如何认定',  \n",
    "    '8妻子下落不明丈夫又与他人登记结婚是否为无效婚姻',  \n",
    "    '9一方未签字办理的结婚登记是否有效',  \n",
    "    '10夫妻双方1990年按农村习俗举办婚礼没有结婚证 一方可否起诉离婚',  \n",
    "    '11结婚前对方父母出资购买的住房写我们二人的名字有效吗',  \n",
    "    '12身份证被别人冒用无法登记结婚怎么办？',  \n",
    "    '13同居后又与他人登记结婚是否构成重婚罪',  \n",
    "    '14未办登记只举办结婚仪式可起诉离婚吗',  \n",
    "    '15同居多年未办理结婚登记，是否可以向法院起诉要求离婚'  \n",
    "]\n",
    "# 分词\n",
    "texts = [[word for word in jieba.cut(document, cut_all=True)] for document in raw_documents]\n",
    "\n",
    "# 建立词典\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(\"The length of dictionary was \", len(dictionary))\n",
    "\n",
    "# 根据词典，对当前语句中的词进行词频统计 即bag-of-words\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# 建立TFIDF\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "# 根据模型计算语料库的TFIDF\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "for i, corpus in enumerate(corpus_tfidf):\n",
    "    print(\"The content of corpus_tifdf[{}] was {}\".format(i, corpus))\n",
    "    print(\"The length of corpus_tfidf[{}] was {}\".format(i, len(corpus)))\n",
    "    print(\"-----------------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "# 创建相似度矩阵 将每个句子生成一个长度为词典大小空间的向量 index.index\n",
    "index = similarities.MatrixSimilarity(corpus_tfidf)\n",
    "\n",
    "# 显示第二个句子的词向量\n",
    "print(index.index[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
